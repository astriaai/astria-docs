"use strict";(self.webpackChunkastria_docs_2=self.webpackChunkastria_docs_2||[]).push([[492],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=r.createContext({}),p=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},u="mdxType",f={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=p(n),d=o,m=u["".concat(s,".").concat(d)]||u[d]||f[d]||a;return n?r.createElement(m,i(i({ref:t},c),{},{components:n})):r.createElement(m,i({ref:t},c))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:o,i[1]=l;for(var p=2;p<a;p++)i[p]=n[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},9511:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>f,frontMatter:()=>a,metadata:()=>l,toc:()=>p});var r=n(7462),o=(n(7294),n(3905));const a={title:"LoRA fine-tuning",description:"LoRA fine-tuning",tags:["LoRA","fine-tuning","style","noise offset","lineart","oil painting"]},i="LoRA",l={unversionedId:"features/lora",id:"features/lora",title:"LoRA fine-tuning",description:"LoRA fine-tuning",source:"@site/docs/features/lora.md",sourceDirName:"features",slug:"/features/lora",permalink:"/docs/features/lora",draft:!1,tags:[{label:"LoRA",permalink:"/docs/tags/lo-ra"},{label:"fine-tuning",permalink:"/docs/tags/fine-tuning"},{label:"style",permalink:"/docs/tags/style"},{label:"noise offset",permalink:"/docs/tags/noise-offset"},{label:"lineart",permalink:"/docs/tags/lineart"},{label:"oil painting",permalink:"/docs/tags/oil-painting"}],version:"current",frontMatter:{title:"LoRA fine-tuning",description:"LoRA fine-tuning",tags:["LoRA","fine-tuning","style","noise offset","lineart","oil painting"]},sidebar:"tutorialSidebar",previous:{title:"Latent Consistency Models",permalink:"/docs/features/lcm"},next:{title:"Multi-Controlnet",permalink:"/docs/features/multi-controlnet"}},s={},p=[],c={toc:p},u="wrapper";function f(e){let{components:t,...n}=e;return(0,o.kt)(u,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"lora"},"LoRA"),(0,o.kt)("p",null,"LoRA (low ranking adaption) is a type of model fine-tuning which takes an additional set of model weights (or parameters) and trains those on top while not changing the existing model. For this reason LoRA training is faster comparing to full checkpoint training as it only trains a small amount of weights comparing to the full model. Additionally, LoRA can be loaded on top of any base model, and multiple LoRA can be combined."),(0,o.kt)("p",null,"The LoRA ecosystem contains many different types of weight structures with different ranks. For this reason some LoRA might not be compatible in different UIs or platforms."),(0,o.kt)("p",null,"LoRAs can be used to enhance the quality of the image or deepen a specific style that is desired."),(0,o.kt)("p",null,"Astria provides a ",(0,o.kt)("a",{parentName:"p",href:"https://www.astria.ai/gallery/tunes?model_type=lora"},"LoRA gallery")," and allows importing external LoRAs. To use a LoRA go to the ",(0,o.kt)("a",{parentName:"p",href:"https://www.astria.ai/prompts"},"generate tab")," and use the LoRA syntax as such: ",(0,o.kt)("inlineCode",{parentName:"p"},"<lora:name:weight>"),". For example, a very common usage ",(0,o.kt)("inlineCode",{parentName:"p"},"<lora:epi_noiseoffset2:0.5>")," - will load a Noise offset lora at strength 0.5 which will turn the image a bit darker and deepen the contrast."),(0,o.kt)("p",null,"See example prompts in the ",(0,o.kt)("a",{parentName:"p",href:"https://www.astria.ai/gallery?text=lora"},"gallery")),(0,o.kt)("p",null,"You can also combine LoRAs as such:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"<lora:epi_noiseoffset2:0.5><lora:FilmVelvia2:0.5><lora:add_detail:0.5><lora:epiCRealismHelper:0.2>\n")),(0,o.kt)("admonition",{type:"warning"},(0,o.kt)("p",{parentName:"admonition"},"Note that LoRA can reduce the similarity of trained subjects when used together. To avoid this use low-strength for the lora.")),(0,o.kt)("p",null,"Some LoRAs may be trained on trigger words which are required to show in the prompt text. Check the LoRA information by clicking on the Website link and reading its docs and trigger words."))}f.isMDXComponent=!0}}]);